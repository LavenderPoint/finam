{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27604d17-a248-4c58-97f5-799529868ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d409fe64-74cd-41f0-823c-c186f66993ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "candles = pd.read_csv('C:/Users/olafe/Downloads/forecast_data/forecast_data/candles.csv')\n",
    "candles_2 = pd.read_csv('C:/Users/olafe/Downloads/forecast_data/forecast_data/candles_2.csv')\n",
    "news = pd.read_csv('C:/Users/olafe/Downloads/forecast_data/forecast_data/news.csv')\n",
    "news_2 = pd.read_csv('C:/Users/olafe/Downloads/forecast_data/forecast_data/news_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71a75d23-bb0e-4156-9039-f8718eca1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ó–ê–ü–£–°–ö –°–ê–ë–ú–ò–¢–ê...\n",
      "Train –¥–∞–Ω–Ω—ã—Ö: 20009 —Å—Ç—Ä–æ–∫\n",
      "Test –¥–∞–Ω–Ω—ã—Ö: 1745 —Å—Ç—Ä–æ–∫\n",
      "–ù–æ–≤–æ—Å—Ç–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã\n",
      "\n",
      "==================================================\n",
      "[FIT] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "[FIT] –û–±—É—á–∞—é—â–∏—Ö —Å—Ç—Ä–æ–∫: 19629\n",
      "[FIT] –ì–æ—Ä–∏–∑–æ–Ω—Ç 1 –¥–Ω–µ–π MAE: 0.014303\n",
      "[FIT] –ì–æ—Ä–∏–∑–æ–Ω—Ç 20 –¥–Ω–µ–π MAE: 0.073874\n",
      "[FIT] –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: model.pkl\n",
      "\n",
      "==================================================\n",
      "[PREDICT] –°–æ–∑–¥–∞–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞...\n",
      "[PREDICT] –°–∞–±–º–∏—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: submission3.csv\n",
      "–†–∞–∑–º–µ—Ä —Å–∞–±–º–∏—Ç–∞: (19, 21)\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\n",
      "  ticker        p1        p2        p3        p4        p5        p6  \\\n",
      "0   AFLT  0.002135  0.003174  0.004212  0.005251  0.006289  0.007327   \n",
      "1   ALRS  0.002009  0.003081  0.004152  0.005223  0.006295  0.007366   \n",
      "2   CHMF  0.001793  0.002912  0.004031  0.005149  0.006268  0.007387   \n",
      "3   GAZP  0.001678  0.002761  0.003844  0.004927  0.006010  0.007093   \n",
      "4   GMKN  0.002430  0.003413  0.004395  0.005378  0.006360  0.007342   \n",
      "\n",
      "         p7        p8        p9  ...       p11       p12       p13       p14  \\\n",
      "0  0.008366  0.009404  0.010443  ...  0.012520  0.013558  0.014597  0.015635   \n",
      "1  0.008438  0.009509  0.010580  ...  0.012723  0.013795  0.014866  0.015937   \n",
      "2  0.008506  0.009624  0.010743  ...  0.012981  0.014099  0.015218  0.016337   \n",
      "3  0.008175  0.009258  0.010341  ...  0.012507  0.013590  0.014673  0.015755   \n",
      "4  0.008325  0.009307  0.010290  ...  0.012254  0.013237  0.014219  0.015202   \n",
      "\n",
      "        p15       p16       p17       p18       p19       p20  \n",
      "0  0.016674  0.017712  0.018751  0.019789  0.020827  0.021866  \n",
      "1  0.017009  0.018080  0.019151  0.020223  0.021294  0.022366  \n",
      "2  0.017456  0.018574  0.019693  0.020812  0.021931  0.023049  \n",
      "3  0.016838  0.017921  0.019004  0.020087  0.021170  0.022253  \n",
      "4  0.016184  0.017166  0.018149  0.019131  0.020114  0.021096  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      " –°–ê–ë–ú–ò–¢ –ì–û–¢–û–í!\n",
      "  –ö–æ–ª–æ–Ω–∫–∏: ['ticker', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20']\n",
      "  –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: 21\n",
      "  –¢–∏–∫–µ—Ä–æ–≤: 19\n",
      "  –§–∞–π–ª: submission3.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "FEATS = [\n",
    "    'momentum_5', 'volatility_5', 'price_range', 'news_count'\n",
    "]\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['begin'] = pd.to_datetime(df['begin'])\n",
    "    df = df.sort_values(['ticker','begin']).reset_index(drop=True)\n",
    "    df['momentum_5'] = df.groupby('ticker')['close'].pct_change(5)\n",
    "    ret1 = df.groupby('ticker')['close'].pct_change()\n",
    "    df['volatility_5'] = ret1.groupby(df['ticker']).rolling(5, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "    df['price_range'] = (df['high'] - df['low']) / df['close']\n",
    "    for col in ['momentum_5','volatility_5','price_range']:\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def add_news_count(df, news):\n",
    "    if news is None or len(news) == 0:\n",
    "        df = df.copy()\n",
    "        df['news_count'] = 0.0\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    news = news.copy()\n",
    "    news['publish_date'] = pd.to_datetime(news['publish_date'])\n",
    "    news['date'] = news['publish_date'].dt.normalize()\n",
    "    daily = news.groupby('date').size().reset_index(name='news_count')\n",
    "    df['date'] = df['begin'].dt.normalize()\n",
    "    df = df.merge(daily, on='date', how='left')\n",
    "    df['news_count'] = df['news_count'].fillna(0.0)\n",
    "    df = df.drop(columns=['date'])\n",
    "    return df\n",
    "\n",
    "def create_targets(df, horizons=(1,20)):\n",
    "    df = df.copy()\n",
    "    for h in horizons:\n",
    "        df[f'target_return_{h}d'] = df.groupby('ticker')['close'].pct_change(h).shift(-h)\n",
    "    return df\n",
    "\n",
    "def fit(train_candles, train_news=None, split_date='2024-09-08', model_path='model.pkl'):\n",
    "    \"\"\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    print(\"[FIT] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "    candles = train_candles.copy()\n",
    "    candles['begin'] = pd.to_datetime(candles['begin'])\n",
    "\n",
    "    cutoff = pd.to_datetime(split_date)\n",
    "    df = candles[candles['begin'] <= cutoff].copy()\n",
    "\n",
    "    df = create_features(df)\n",
    "    df = add_news_count(df, train_news)\n",
    "    df = create_targets(df)\n",
    "\n",
    "    mask = ~df[[f'target_return_{h}d' for h in (1,20)]].isna().any(axis=1)\n",
    "    df_train = df.loc[mask].reset_index(drop=True)\n",
    "    print(f\"[FIT] –û–±—É—á–∞—é—â–∏—Ö —Å—Ç—Ä–æ–∫: {len(df_train)}\")\n",
    "\n",
    "    X = df_train[FEATS].values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "\n",
    "    models = {}\n",
    "    for h in (1,20):\n",
    "        y = df_train[f'target_return_{h}d'].values\n",
    "        reg = LinearRegression().fit(Xs, y)\n",
    "        models[f'reg_{h}'] = reg\n",
    "        mae = float(np.mean(np.abs(y - reg.predict(Xs))))\n",
    "        print(f\"[FIT] –ì–æ—Ä–∏–∑–æ–Ω—Ç {h} –¥–Ω–µ–π MAE: {mae:.6f}\")\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({'features': FEATS, 'scaler': scaler, 'models': models}, f)\n",
    "    print(f\"[FIT] –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}\")\n",
    "\n",
    "def predict(test_candles, test_news=None, model_path='model.pkl', output_path='submission2.csv'):\n",
    "    \"\"\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞\"\"\"\n",
    "    print(\"[PREDICT] –°–æ–∑–¥–∞–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞...\")\n",
    "\n",
    "    candles = test_candles.copy()\n",
    "    candles['begin'] = pd.to_datetime(candles['begin'])\n",
    "\n",
    "    with open(model_path, 'rb') as f:\n",
    "        payload = pickle.load(f)\n",
    "    feats = payload['features']\n",
    "    scaler = payload['scaler']\n",
    "    models = payload['models']\n",
    "\n",
    "    df = create_features(candles)\n",
    "    df = add_news_count(df, test_news)\n",
    "\n",
    "    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–∫–µ—Ä—É\n",
    "    last_idx = df.groupby('ticker')['begin'].idxmax()\n",
    "    dfl = df.loc[last_idx].reset_index(drop=True)\n",
    "\n",
    "    Xs = scaler.transform(dfl[feats].values)\n",
    "    pr1  = models['reg_1'].predict(Xs)\n",
    "    pr20 = models['reg_20'].predict(Xs)\n",
    "\n",
    "    # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤\n",
    "    alphas = np.linspace(0, 1, 20)[:, None]\n",
    "    band = (1 - alphas) * pr1[None, :] + alphas * pr20[None, :]\n",
    "    band = np.clip(band, -0.5, 0.5).T\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–∞–±–º–∏—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ: —Ç–æ–ª—å–∫–æ ticker –∏ p1-p20\n",
    "    submission = pd.DataFrame()\n",
    "    submission['ticker'] = dfl['ticker']\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º p1-p20 (—Ç–æ–ª—å–∫–æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏)\n",
    "    for i in range(20):\n",
    "        submission[f'p{i+1}'] = np.round(band[:, i], 6)\n",
    "\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"[PREDICT] –°–∞–±–º–∏—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä —Å–∞–±–º–∏—Ç–∞: {submission.shape}\")\n",
    "    print(\"\\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\")\n",
    "    print(submission.head())\n",
    "    return submission\n",
    "\n",
    "# ===== –ó–ê–ü–£–°–ö –í –ù–û–£–¢–ë–£–ö–ï =====\n",
    "if __name__ == \"__main__\":\n",
    "    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ —Å —Ç–≤–æ–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "    print(\"üöÄ –ó–ê–ü–£–°–ö –°–ê–ë–ú–ò–¢–ê...\")\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–µ\n",
    "    split_date = '2024-09-08'\n",
    "\n",
    "    candles['begin'] = pd.to_datetime(candles['begin'])\n",
    "    candles_2['begin'] = pd.to_datetime(candles_2['begin'])\n",
    "\n",
    "    # candles - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ (–¥–æ 8 —Å–µ–Ω—Ç—è–±—Ä—è)\n",
    "    train_candles = candles[candles['begin'] < split_date].copy()\n",
    "    # candles_2 - —Ç–µ—Å—Ç–æ–≤—ã–µ (–ø–æ—Å–ª–µ 8 —Å–µ–Ω—Ç—è–±—Ä—è)\n",
    "    test_candles = candles_2[candles_2['begin'] >= split_date].copy()\n",
    "\n",
    "    print(f\"Train –¥–∞–Ω–Ω—ã—Ö: {len(train_candles)} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"Test –¥–∞–Ω–Ω—ã—Ö: {len(test_candles)} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏\n",
    "    if 'news' in locals() and 'news_2' in locals():\n",
    "        train_news = news\n",
    "        test_news = news_2\n",
    "        print(\"–ù–æ–≤–æ—Å—Ç–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã\")\n",
    "    else:\n",
    "        train_news = None\n",
    "        test_news = None\n",
    "        print(\"–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è\")\n",
    "\n",
    "    # 1. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    fit(train_candles, train_news, split_date=split_date)\n",
    "\n",
    "    # 2. –°–æ–∑–¥–∞–µ–º —Å–∞–±–º–∏—Ç\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    submission = predict(test_candles, test_news)\n",
    "\n",
    "    print(\"\\n –°–ê–ë–ú–ò–¢ –ì–û–¢–û–í!\")\n",
    "    print(f\"  –ö–æ–ª–æ–Ω–∫–∏: {list(submission.columns)}\")\n",
    "    print(f\"  –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(submission.columns)}\")\n",
    "    print(f\"  –¢–∏–∫–µ—Ä–æ–≤: {len(submission['ticker'].unique())}\")\n",
    "    print(f\"  –§–∞–π–ª: submission2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed87e4c-7c71-4d9c-9868-80036c01c8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
